# Web Crawler
-------------

TODO:

- set User-Agent Header to mimic a real browser
- Map a web content to another website
- Implement Robots Exclusion Protocol (read robots.txt)

# Crawling Policy

- a selection policy which states the pages to download,
- a re-visit policy which states when to check for changes to the pages,
- a politeness policy that states how to avoid overloading Web sites.
- a parallelization policy that states how to coordinate distributed web crawlers.
