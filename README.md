# Web Crawler
-------------

TODO:

1: Start with seed. A web crawler starts with a list of URLs to visit. Make the seed function get information of an specific url.

2: From the seed function make a function that take the url specific content and display in another place (your web localhost).

3: Thus will form the crawler, and for the crawler function (using those 2 function above) we make a function that downloads the web page (with limited number of web pages
within a given time). that what make the crawler.

# Crawling Policy

- a selection policy which states the pages to download,
- a re-visit policy which states when to check for changes to the pages,
- a politeness policy that states how to avoid overloading Web sites.
- a parallelization policy that states how to coordinate distributed web crawlers.


